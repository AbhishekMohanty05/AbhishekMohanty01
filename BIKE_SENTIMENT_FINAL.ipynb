{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRlYPO5wnOuH5VBrxBR2em",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekMohanty05/AbhishekMohanty01/blob/main/BIKE_SENTIMENT_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0vU0VfYKpv-K",
        "outputId": "971d1d9f-25d6-4e58-98b5-72a850ae379c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-caaca7fa1d02>:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['review_description'].fillna('', inplace=True)  # Replace null strings with an empty string\n",
            "<ipython-input-10-caaca7fa1d02>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['label'].fillna(-1, inplace=True)  # Replace null labels with -1 (to be dropped later)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 20\n",
            "\n",
            "Training Loss: 0.5005368662731988\n",
            "Validation Loss: 0.3291325490725668\n",
            "\n",
            " Epoch 2 / 20\n",
            "\n",
            "Training Loss: 0.28157143958977293\n",
            "Validation Loss: 0.27782436028907176\n",
            "\n",
            " Epoch 3 / 20\n",
            "\n",
            "Training Loss: 0.19558065263288363\n",
            "Validation Loss: 0.21199389101055108\n",
            "\n",
            " Epoch 4 / 20\n",
            "\n",
            "Training Loss: 0.12045260324542012\n",
            "Validation Loss: 0.2087827538336186\n",
            "\n",
            " Epoch 5 / 20\n",
            "\n",
            "Training Loss: 0.07540114404899734\n",
            "Validation Loss: 0.22441889710822388\n",
            "\n",
            " Epoch 6 / 20\n",
            "\n",
            "Training Loss: 0.046761112364807296\n",
            "Validation Loss: 0.21685496376410715\n",
            "\n",
            " Epoch 7 / 20\n",
            "\n",
            "Training Loss: 0.02404975540802947\n",
            "Validation Loss: 0.20275305710904495\n",
            "\n",
            " Epoch 8 / 20\n",
            "\n",
            "Training Loss: 0.016418407391424156\n",
            "Validation Loss: 0.20240732301087855\n",
            "\n",
            " Epoch 9 / 20\n",
            "\n",
            "Training Loss: 0.011751963909316276\n",
            "Validation Loss: 0.20976150938459845\n",
            "\n",
            " Epoch 10 / 20\n",
            "\n",
            "Training Loss: 0.010402993475519387\n",
            "Validation Loss: 0.23585157766616535\n",
            "\n",
            " Epoch 11 / 20\n",
            "\n",
            "Training Loss: 0.01316359825789862\n",
            "Validation Loss: 0.2564574230656083\n",
            "\n",
            " Epoch 12 / 20\n",
            "\n",
            "Training Loss: 0.006101729543297551\n",
            "Validation Loss: 0.2047225423157215\n",
            "\n",
            " Epoch 13 / 20\n",
            "\n",
            "Training Loss: 0.003961074737308081\n",
            "Validation Loss: 0.26974381606257675\n",
            "\n",
            " Epoch 14 / 20\n",
            "\n",
            "Training Loss: 0.010259175820891479\n",
            "Validation Loss: 0.2835723765109785\n",
            "\n",
            " Epoch 15 / 20\n",
            "\n",
            "Training Loss: 0.012911407550688767\n",
            "Validation Loss: 0.21678606168528572\n",
            "\n",
            " Epoch 16 / 20\n",
            "\n",
            "Training Loss: 0.007981558872187244\n",
            "Validation Loss: 0.23247165331734945\n",
            "\n",
            " Epoch 17 / 20\n",
            "\n",
            "Training Loss: 0.010259381893598141\n",
            "Validation Loss: 0.2678184037781887\n",
            "\n",
            " Epoch 18 / 20\n",
            "\n",
            "Training Loss: 0.0033204613221044254\n",
            "Validation Loss: 0.2894359634684454\n",
            "\n",
            " Epoch 19 / 20\n",
            "\n",
            "Training Loss: 0.01006613845024341\n",
            "Validation Loss: 0.2553538176355352\n",
            "\n",
            " Epoch 20 / 20\n",
            "\n",
            "Training Loss: 0.006664915987930726\n",
            "Validation Loss: 0.2722323593864674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-caaca7fa1d02>:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('saved_weights.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.96       600\n",
            "           1       0.99      0.94      0.96       600\n",
            "\n",
            "    accuracy                           0.96      1200\n",
            "   macro avg       0.97      0.96      0.96      1200\n",
            "weighted avg       0.97      0.96      0.96      1200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import resample, class_weight\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/Bike_Reviews_Final_1.csv', encoding='Windows-1252') # or 'latin-1', 'ISO-8859-1'\n",
        "\n",
        "# Check for null values and handle them\n",
        "df['review_description'].fillna('', inplace=True)  # Replace null strings with an empty string\n",
        "df['label'].fillna(-1, inplace=True)  # Replace null labels with -1 (to be dropped later)\n",
        "\n",
        "# Ensure correct datatypes\n",
        "df['review_description'] = df['review_description'].astype(str)\n",
        "df['label'] = pd.to_numeric(df['label'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "# Drop rows with invalid labels (-1)\n",
        "df = df[df['label'] != -1]\n",
        "\n",
        "# Balance the dataset to have 4000 samples for each class\n",
        "df_positive = df[df['label'] == 1].sample(n=4000, random_state=42)  # Downsample positive to 4000\n",
        "df_negative = resample(df[df['label'] == 0], replace=True, n_samples=4000, random_state=42)  # Oversample negative to 4000\n",
        "df = pd.concat([df_positive, df_negative]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Basic statistics for text lengths\n",
        "lens = [len(i.split()) for i in df['review_description']]\n",
        "plt.hist(lens)\n",
        "plt.show()\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['review_description'], df['label'],\n",
        "                                                                    random_state=2021, test_size=0.3,\n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2021, test_size=0.5,\n",
        "                                                                stratify=temp_labels)\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define padding length based on text lengths\n",
        "pad_len = 17\n",
        "\n",
        "# Tokenize and encode sequences\n",
        "tokens_train = tokenizer.batch_encode_plus(train_text.tolist(), max_length=pad_len, padding='max_length', truncation=True)\n",
        "tokens_val = tokenizer.batch_encode_plus(val_text.tolist(), max_length=pad_len, padding='max_length', truncation=True)\n",
        "tokens_test = tokenizer.batch_encode_plus(test_text.tolist(), max_length=pad_len, padding='max_length', truncation=True)\n",
        "\n",
        "# Convert tokens to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "# Define dataloaders\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "# Define the BERT-based model architecture\n",
        "class BERT_architecture(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_architecture, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate and move model to device\n",
        "model = BERT_architecture(bert)\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer and weighted loss function\n",
        "optimizer = transformers.AdamW(model.parameters(), lr=1e-5)\n",
        "class_weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "cross_entropy = nn.NLLLoss(weight=weights)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_preds = []\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        model.zero_grad()\n",
        "        preds = model(sent_id, mask)\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        total_preds.append(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds\n",
        "\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_preds = []\n",
        "\n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        with torch.no_grad():\n",
        "            preds = model(sent_id, mask)\n",
        "            loss = cross_entropy(preds, labels)\n",
        "            total_loss += loss.item()\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\n",
        "    return avg_loss, total_preds\n",
        "\n",
        "# Train and evaluate the model\n",
        "epochs = 20\n",
        "best_valid_loss = float('inf')\n",
        "train_losses, valid_losses = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'\\n Epoch {epoch + 1} / {epochs}')\n",
        "    train_loss, _ = train()\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss}')\n",
        "    print(f'Validation Loss: {valid_loss}')\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(torch.load('saved_weights.pt'))\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n",
        "pred = np.argmax(preds, axis=1)\n",
        "print(classification_report(test_y, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqPWzyiLyLeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_TeW6r8qSbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enhMmnzJqgaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_v5o3GTvw7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}